{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "import streamlit as st\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import unittest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class YouTubeCommentScraper:\n",
    "    def __init__(self):\n",
    "        self.headers = {\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'\n",
    "        }\n",
    "    \n",
    "    def extract_video_id(self, url):\n",
    "        \"\"\"Extrae el ID del video de una URL de YouTube\"\"\"\n",
    "        patterns = [\n",
    "            r'(?:v=|\\/)([0-9A-Za-z_-]{11}).*',\n",
    "            r'(?:embed\\/)([0-9A-Za-z_-]{11})',\n",
    "            r'(?:youtu\\.be\\/)([0-9A-Za-z_-]{11})'\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.search(pattern, url)\n",
    "            if match:\n",
    "                return match.group(1)\n",
    "        return None\n",
    "    \n",
    "    def get_comments(self, video_url, max_comments=100):\n",
    "        \"\"\"Obtiene comentarios de un video de YouTube\"\"\"\n",
    "        video_id = self.extract_video_id(video_url)\n",
    "        if not video_id:\n",
    "            raise ValueError(\"URL de video inválida\")\n",
    "            \n",
    "        # Aquí normalmente usaríamos la API de YouTube, pero por simplicidad\n",
    "        # simularemos algunos comentarios de ejemplo\n",
    "        # En un entorno real, necesitarías configurar la API de YouTube\n",
    "        sample_comments = [\n",
    "            \"Este es un comentario de ejemplo 1\",\n",
    "            \"Este es un comentario de ejemplo 2\",\n",
    "            \"Este es un comentario de ejemplo 3\"\n",
    "        ]\n",
    "        \n",
    "        return sample_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnhancedHateSpeechDetector:\n",
    "    def __init__(self):\n",
    "        # Crear clasificadores base\n",
    "        clf1 = LogisticRegression(C=0.1, class_weight='balanced', random_state=42)\n",
    "        clf2 = MultinomialNB(alpha=0.1)\n",
    "        clf3 = LinearSVC(C=0.1, class_weight='balanced', random_state=42)\n",
    "        \n",
    "        # Crear ensemble\n",
    "        ensemble = VotingClassifier(\n",
    "            estimators=[\n",
    "                ('lr', clf1),\n",
    "                ('nb', clf2),\n",
    "                ('svc', clf3)\n",
    "            ],\n",
    "            voting='soft'\n",
    "        )\n",
    "        \n",
    "        # Crear pipeline\n",
    "        self.pipeline = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(\n",
    "                max_features=3000,\n",
    "                ngram_range=(1, 2),\n",
    "                min_df=2,\n",
    "                max_df=0.95,\n",
    "                stop_words='english'\n",
    "            )),\n",
    "            ('scaler', StandardScaler(with_mean=False)),\n",
    "            ('ensemble', ensemble)\n",
    "        ])\n",
    "        \n",
    "        self.scraper = YouTubeCommentScraper()\n",
    "    \n",
    "    def prepare_target(self, df):\n",
    "        \"\"\"Combina todas las columnas objetivo en una sola etiqueta de odio\"\"\"\n",
    "        hate_columns = ['IsToxic', 'IsAbusive', 'IsThreat', 'IsProvocative', \n",
    "                       'IsObscene', 'IsHatespeech', 'IsRacist', 'IsNationalist', \n",
    "                       'IsSexist', 'IsHomophobic', 'IsReligiousHate', 'IsRadicalism']\n",
    "        return (df[hate_columns].sum(axis=1) > 0).astype(int)\n",
    "    \n",
    "    def train(self, df):\n",
    "        \"\"\"Entrena el modelo con los datos proporcionados\"\"\"\n",
    "        X = df['Text']\n",
    "        y = self.prepare_target(df)\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        self.pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        train_pred = self.pipeline.predict(X_train)\n",
    "        test_pred = self.pipeline.predict(X_test)\n",
    "        \n",
    "        train_acc = np.mean(train_pred == y_train)\n",
    "        test_acc = np.mean(test_pred == y_test)\n",
    "        \n",
    "        return {\n",
    "            'train_accuracy': train_acc,\n",
    "            'test_accuracy': test_acc,\n",
    "            'overfitting': train_acc - test_acc,\n",
    "            'classification_report': classification_report(y_test, test_pred)\n",
    "        }\n",
    "    \n",
    "    def analyze_video(self, video_url):\n",
    "        \"\"\"Analiza los comentarios de un video de YouTube\"\"\"\n",
    "        try:\n",
    "            comments = self.scraper.get_comments(video_url)\n",
    "            predictions = self.predict(comments)\n",
    "            \n",
    "            results = []\n",
    "            for comment, pred_prob in zip(comments, predictions):\n",
    "                results.append({\n",
    "                    'comment': comment,\n",
    "                    'hate_probability': pred_prob,\n",
    "                    'is_hate': pred_prob > 0.5\n",
    "                })\n",
    "            \n",
    "            return results\n",
    "        except Exception as e:\n",
    "            raise Exception(f\"Error al analizar el video: {str(e)}\")\n",
    "    \n",
    "    def predict(self, texts):\n",
    "        \"\"\"Predice si los textos contienen mensajes de odio\"\"\"\n",
    "        if isinstance(texts, str):\n",
    "            texts = [texts]\n",
    "        return self.pipeline.predict_proba(texts)[:, 1]\n",
    "    \n",
    "    def save_model(self, path):\n",
    "        \"\"\"Guarda el modelo entrenado\"\"\"\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump(self.pipeline, f)\n",
    "    \n",
    "    @classmethod\n",
    "    def load_model(cls, path):\n",
    "        \"\"\"Carga un modelo guardado\"\"\"\n",
    "        detector = cls()\n",
    "        with open(path, 'rb') as f:\n",
    "            detector.pipeline = pickle.load(f)\n",
    "        return detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests unitarios\n",
    "class TestHateSpeechDetector(unittest.TestCase):\n",
    "    def setUp(self):\n",
    "        self.detector = EnhancedHateSpeechDetector()\n",
    "        self.sample_text = \"Este es un texto de prueba\"\n",
    "        self.sample_url = \"https://www.youtube.com/watch?v=dQw4w9WgXcQ\"\n",
    "    \n",
    "    def test_prediction_format(self):\n",
    "        pred = self.detector.predict(self.sample_text)\n",
    "        self.assertTrue(0 <= pred <= 1)\n",
    "    \n",
    "    def test_video_id_extraction(self):\n",
    "        video_id = self.detector.scraper.extract_video_id(self.sample_url)\n",
    "        self.assertEqual(len(video_id), 11)\n",
    "    \n",
    "    def test_video_analysis_format(self):\n",
    "        results = self.detector.analyze_video(self.sample_url)\n",
    "        self.assertTrue(isinstance(results, list))\n",
    "        if results:\n",
    "            self.assertTrue(all(isinstance(r, dict) for r in results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interfaz Streamlit mejorada\n",
    "def create_streamlit_app():\n",
    "    st.title(\"Detector de Mensajes de Odio\")\n",
    "    \n",
    "    try:\n",
    "        detector = EnhancedHateSpeechDetector.load_model('hate_speech_model_enhanced.pkl')\n",
    "    except:\n",
    "        st.error(\"No se encontró un modelo entrenado. Por favor, entrene el modelo primero.\")\n",
    "        return\n",
    "    \n",
    "    # Tabs para diferentes funcionalidades\n",
    "    tab1, tab2 = st.tabs([\"Analizar Texto\", \"Analizar Video de YouTube\"])\n",
    "    \n",
    "    with tab1:\n",
    "        text_input = st.text_area(\"Introduce el texto a analizar:\")\n",
    "        if st.button(\"Analizar Texto\"):\n",
    "            if text_input:\n",
    "                probability = detector.predict(text_input)[0]\n",
    "                st.write(f\"Probabilidad de contenido de odio: {probability:.2%}\")\n",
    "                \n",
    "                if probability > 0.5:\n",
    "                    st.error(\"⚠️ Este texto puede contener mensajes de odio.\")\n",
    "                else:\n",
    "                    st.success(\"✅ Este texto parece seguro.\")\n",
    "    \n",
    "    with tab2:\n",
    "        video_url = st.text_input(\"Introduce la URL del video de YouTube:\")\n",
    "        if st.button(\"Analizar Video\"):\n",
    "            if video_url:\n",
    "                with st.spinner(\"Analizando comentarios...\"):\n",
    "                    try:\n",
    "                        results = detector.analyze_video(video_url)\n",
    "                        \n",
    "                        hate_comments = [r for r in results if r['is_hate']]\n",
    "                        st.write(f\"Se encontraron {len(hate_comments)} comentarios potencialmente ofensivos de {len(results)} analizados.\")\n",
    "                        \n",
    "                        for result in results:\n",
    "                            if result['is_hate']:\n",
    "                                st.error(f\"⚠️ {result['comment']}\\nProbabilidad: {result['hate_probability']:.2%}\")\n",
    "                            else:\n",
    "                                st.success(f\"✅ {result['comment']}\\nProbabilidad: {result['hate_probability']:.2%}\")\n",
    "                    except Exception as e:\n",
    "                        st.error(f\"Error al analizar el video: {str(e)}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    df = pd.read_csv('youtoxic_english_1000.csv')\n",
    "    detector = EnhancedHateSpeechDetector()\n",
    "    metrics = detector.train(df)\n",
    "    print(\"Métricas del modelo:\")\n",
    "    for key, value in metrics.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    detector.save_model('hate_speech_model_enhanced.pkl')\n",
    "    \n",
    "    create_streamlit_app()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.1.3 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/runpy.py\", line 198, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/runpy.py\", line 88, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 638, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/base_events.py\", line 1971, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/asyncio/events.py\", line 84, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/6l/xc4cvh3j3zj9b4f6zl1r0m100000gn/T/ipykernel_62739/4229399382.py\", line 1, in <module>\n",
      "    import torch\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/torch/__init__.py\", line 1477, in <module>\n",
      "    from .functional import *  # noqa: F403\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/torch/functional.py\", line 9, in <module>\n",
      "    import torch.nn.functional as F\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/torch/nn/__init__.py\", line 1, in <module>\n",
      "    from .modules import *  # noqa: F403\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/torch/nn/modules/__init__.py\", line 35, in <module>\n",
      "    from .transformer import TransformerEncoder, TransformerDecoder, \\\n",
      "  File \"/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/torch/nn/modules/transformer.py\", line 20, in <module>\n",
      "    device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n",
      "/Users/cash/Desktop/F5/Python/NLP/env/lib/python3.12/site-packages/torch/nn/modules/transformer.py:20: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/tensor_numpy.cpp:84.)\n",
      "  device: torch.device = torch.device(torch._C._get_default_device()),  # torch.device('cpu'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU disponible: False\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"GPU disponible:\", torch.cuda.is_available())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
